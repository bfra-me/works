# Benchmark regression detection workflow for @bfra.me/es package.
---
name: Benchmarks

on:
  schedule:
    - cron: '0 18 * * 6' # Saturday 6PM UTC - Weekly performance tracking
  pull_request:
    branches: [main]
    paths:
      - packages/es/**
      - .github/workflows/benchmarks.yaml
  push:
    branches: [main]
    paths:
      - packages/es/**
      - .github/workflows/benchmarks.yaml
  workflow_dispatch:
    inputs:
      update_baseline:
        description: Update the baseline with current benchmark results
        required: false
        default: false
        type: boolean

concurrency:
  group: ${{ github.workflow }}-${{ github.event.number || github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

permissions:
  contents: read

jobs:
  benchmark:
    name: Performance Benchmarks
    permissions:
      contents: read
      pull-requests: write
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1

      - name: Prepare job
        uses: ./.github/actions/pnpm-install

      - name: Set build mode
        id: build-mode
        run: |
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            echo "mode=filtered" >> $GITHUB_OUTPUT
            echo "command=pnpm --filter @bfra.me/es^... --filter @bfra.me/es run build" >> $GITHUB_OUTPUT
          else
            echo "mode=full" >> $GITHUB_OUTPUT
            echo "command=pnpm run build" >> $GITHUB_OUTPUT
          fi

      - name: Build packages (${{ steps.build-mode.outputs.mode }})
        run: ${{ steps.build-mode.outputs.command }}

      - name: Run benchmarks
        id: bench
        working-directory: packages/es
        env:
          BENCHMARK_MODE: ${{ github.event_name == 'pull_request' && 'fast' || 'full' }}
        run: |
          # Run benchmarks and capture output
          echo "Running benchmarks in $BENCHMARK_MODE mode"
          pnpm bench 2>&1 | tee benchmark-output.txt
          echo "Benchmarks completed"

      - name: Run regression check
        if: ${{ !inputs.update_baseline }}
        id: regression
        working-directory: packages/es
        continue-on-error: true
        env:
          REGRESSION_THRESHOLD: ${{ github.event_name == 'pull_request' && '15' || '10' }}
        run: |
          # Run the benchmark comparison script
          echo "Using regression threshold: $REGRESSION_THRESHOLD%"
          pnpm bench:ci 2>&1 | tee regression-output.txt || true

          # Check if comparison was performed
          if grep -q "OVERALL: ‚úì PASSED" regression-output.txt; then
            echo "result=passed" >> "$GITHUB_OUTPUT"
          elif grep -q "OVERALL: ‚úó FAILED" regression-output.txt; then
            echo "result=failed" >> "$GITHUB_OUTPUT"
          else
            echo "result=skipped" >> "$GITHUB_OUTPUT"
          fi

      - name: Update baseline
        if: ${{ inputs.update_baseline || (github.event_name == 'push' && github.ref == 'refs/heads/main') }}
        working-directory: packages/es
        run: |
          echo "Updating baseline with current benchmark results..."
          pnpm bench:update || echo "Baseline update requires Vitest JSON output support"

      - id: get-workflow-access-token
        if: ${{ github.event_name == 'pull_request' && steps.regression.outputs.result != 'skipped' && github.repository == 'bfra-me/works' }}
        name: Get Workflow Access Token
        uses: actions/create-github-app-token@7e473efe3cb98aa54f8d4bac15400b15fad77d94 # v2.2.0
        with:
          app-id: ${{ secrets.APPLICATION_ID }}
          permission-pull-requests: write
          private-key: ${{ secrets.APPLICATION_PRIVATE_KEY }}

      - name: Comment on PR
        if: ${{ github.event_name == 'pull_request' && steps.regression.outputs.result != 'skipped' }}
        uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd # v8
        with:
          github-token: ${{ steps.get-workflow-access-token.outputs.token }}
          script: |
            const fs = require('fs');

            // Read benchmark output
            let benchmarkOutput = '';
            try {
              benchmarkOutput = fs.readFileSync('packages/es/benchmark-output.txt', 'utf8');
            } catch (e) {
              benchmarkOutput = 'Benchmark output not available';
            }

            // Read regression output
            let regressionOutput = '';
            try {
              regressionOutput = fs.readFileSync('packages/es/regression-output.txt', 'utf8');
            } catch (e) {
              regressionOutput = 'Regression check output not available';
            }

            const result = '${{ steps.regression.outputs.result }}';
            const icon = result === 'passed' ? '‚úÖ' : result === 'failed' ? '‚ùå' : '‚ö†Ô∏è';
            const status = result === 'passed' ? 'PASSED' : result === 'failed' ? 'FAILED' : 'SKIPPED';

            // Create collapsible sections for the output
            const body = `## ${icon} Benchmark Results

            **Status:** ${status}
            **Regression Threshold:** ‚â§${{ github.event_name == 'pull_request' && '15' || '10' }}%

            <details>
            <summary>üìä Benchmark Output</summary>

            \`\`\`
            ${benchmarkOutput.slice(0, 10000)}
            \`\`\`

            </details>

            <details>
            <summary>üìà Regression Analysis</summary>

            \`\`\`
            ${regressionOutput.slice(0, 10000)}
            \`\`\`

            </details>

            ---
            *Benchmark regression detection for @bfra.me/es package*
            `;

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Benchmark Results')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body.trim(),
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body.trim(),
              });
            }

      - name: Generate performance trend report
        if: ${{ github.event_name == 'schedule' }}
        working-directory: packages/es
        run: |
          echo "=== Performance Trend Report ===" | tee trend-report.txt
          echo "Generated: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" | tee -a trend-report.txt
          echo "Commit: $(git rev-parse --short HEAD)" | tee -a trend-report.txt
          echo "" | tee -a trend-report.txt
          echo "This is a scheduled performance tracking run." | tee -a trend-report.txt
          echo "Results are compared against the baseline for regression detection." | tee -a trend-report.txt
          echo "" | tee -a trend-report.txt
          echo "Future enhancement: Historical trend analysis across multiple runs" | tee -a trend-report.txt

      - name: Upload benchmark results
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        with:
          name: benchmark-results-${{ github.event_name == 'schedule' && format('scheduled-{0}', github.run_number) || 'pr' }}
          path: |
            packages/es/benchmark-output.txt
            packages/es/regression-output.txt
            packages/es/trend-report.txt
            packages/es/test/benchmarks/baselines/baseline.json
          retention-days: ${{ github.event_name == 'schedule' && 90 || 30 }}
          if-no-files-found: ignore

      - name: Check regression result
        if: ${{ steps.regression.outputs.result == 'failed' }}
        run: |
          THRESHOLD=${{ github.event_name == 'pull_request' && '15' || '10' }}
          echo "::warning::Performance regression detected! Some benchmarks exceeded the ${THRESHOLD}% threshold."
          echo "Please review the benchmark results and consider optimizing the affected code."
          # Note: We use warning instead of failing the build because benchmarks can be noisy
          # To make this a hard failure, uncomment the following line:
          # exit 1
